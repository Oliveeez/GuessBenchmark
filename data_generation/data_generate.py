import re
import json
from openai import OpenAI

from dotenv import load_dotenv
import os

load_dotenv()

# Initialize OpenAI client
client = OpenAI(api_key=os.getenv("DS_API_KEY"), base_url=os.getenv("DS_URL"))

# File paths
# input_file = "./pop_parted_idioms_origin/idioms_1_500.json"  
# compliant_output_file = "./pop_idioms_emoji_pair/idiom_emoji_1_500_by_ds.json"  
# error_output_file = "./pop_idioms_emoji_pair/idiom_emoji_1_500_by_ds_error.json" 
 
input_file = "/data/tianyu_data/appendix/GuessBenchmark/data_generation/pop_idioms_merged.json"  
compliant_output_file = "./pop_idioms_emoji_pair/idiom_emoji_merged_by_ds.json"  
error_output_file = "./pop_idioms_emoji_pair/idiom_emoji_merged_by_ds_error.json" 

# Regular expression pattern to extract JSON from response
json_pattern = re.compile(r'```json\s*({.*?})\s*```', re.DOTALL)

def process_response(idiom, response_content):
    """Process the API response and save to appropriate file based on content."""
    # Try to extract JSON from the response
    json_match = json_pattern.search(response_content)
    
    if json_match:
        try:
            # Parse the JSON to validate it
            response_json = json.loads(json_match.group(1))
            if "emoji_rep" in response_json and "inference_chain" in response_json:
                # Create new JSON with idiom as first element
                compliant_data = {
                    "idiom": idiom,
                    "emoji_rep": response_json["emoji_rep"],
                    "inference_chain": response_json["inference_chain"]
                }
                # Valid format found
                with open(compliant_output_file, 'a', encoding='utf-8') as f:
                    json.dump(compliant_data, f, ensure_ascii=False)
                    f.write('\n') 
                return True
        except json.JSONDecodeError:
            pass
    
    # If we get here, either no JSON found or invalid format
    fail_data = {
        "idiom": idiom,
        "response": response_content
    }
    with open(error_output_file, 'a', encoding='utf-8') as f:
        json.dump(fail_data, f, ensure_ascii=False)
        f.write('\n')  
    return False

def main():
    # Read idioms from file
    try:
        with open(input_file, 'r', encoding='utf-8') as f:
            data = json.load(f)  
            data = data[5115:]
            print(f"===Total idioms to process: {len(data)}===")
            idioms = [item['word'] for item in data if 'word' in item]  
    except FileNotFoundError:
        print(f"Error: Input file '{input_file}' not found.")
        return
    except json.JSONDecodeError:
        print(f"Error: File '{input_file}' is not a valid JSON file.")
        return
    except KeyError:
        print(f"Error: Some entries in '{input_file}' are missing the 'word' field.")
        return
    
    # Process each idiom with progress tracking
    for index, idiom in enumerate(idioms, start=1):
        try:
            # Print progress
            print(f"Processing idiom {index}/{len(idioms)}: {idiom}", end=" - ")
            
            response = client.chat.completions.create(
                model="deepseek-chat",
            #     messages=[
            #         {
            #             "role": "system", 
            #             "content": (
            #                 "You are a helpful assistant, capable of generating emoji representation sets for Chinese idioms. "
            #                 "Each set must contain exactly four emojis, with each emoji strictly corresponding to one character in the idiom IN SEQUENTIAL ORDER. "
            #                 "The corresponding emojis can follow either of these two rules: 1) The emoji represents a meaning that aligns with the character's meaning, or "
            #                 "2) The emoji represents an object whose Chinese pronunciation (pinyin) is identical or very similar to the character's pronunciation. "
            #                 "You must apply ONE of these two rules INDEPENDENTLY for each individual character in the idiom. "
            #                 "Additionally, your output must include both the final emoji result and the reasoning process behind generating the emoji set. "
            #                 "The output must be a single JSON object containing only the JSON and no additional text. "
            #                 "The JSON format should be: {\"emoji_rep\": \"xxxx\", \"inference_chain\": \"...\"}."
            #             )
            #         },
            #         {
            #             "role": "user", 
            #             "content": f"Please generate a set of emojis for the idiom '{idiom}'"
            #         },
            #     ],
            #     stream=False
            # )
                messages=[
                    {
                        "role": "system", 
                        "content": (
                            "ä½ æ˜¯ä¸€ä¸ªæ“…é•¿å°†ä¸­æ–‡æˆè¯­è½¬åŒ–ä¸ºemojiè¡¨æƒ…ç¬¦å·çš„æ™ºèƒ½åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹è¦æ±‚ç”Ÿæˆç»“æœï¼š"
                            "ã€æ ¸å¿ƒè§„åˆ™ã€‘1. æ¯ä¸ªæˆè¯­å¿…é¡»ç”Ÿæˆç”±4ä¸ªè¡¨æƒ…ç¬¦å·ç»„æˆçš„åºåˆ— 2. æ¯ä¸ªè¡¨æƒ…å¿…é¡»ä¸¥æ ¼å¯¹åº”æˆè¯­ä¸­çš„æ±‰å­—é¡ºåº 3. è¡¨æƒ…é€‰æ‹©éœ€æ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼š"
                            "â–¶ å«ä¹‰åŒ¹é…ï¼šè¡¨æƒ…è±¡å¾æ„ä¹‰ä¸å¯¹åº”æ±‰å­—å«ä¹‰ç›¸ç¬¦ â–¶ å‘éŸ³åŒ¹é…ï¼šè¡¨æƒ…ä¸­æ–‡è¯»éŸ³ï¼ˆæ‹¼éŸ³ï¼‰ä¸æ±‰å­—å‘éŸ³ç›¸åŒ/é«˜åº¦ç›¸ä¼¼"
                            "ã€å…³é”®è¦æ±‚ã€‘1. æ¯ä¸ªå­—ç¬¦å•ç‹¬é€‰æ‹©å…¶ä¸­ä¸€ç§åŒ¹é…è§„åˆ™ 2. å¿…é¡»åŒæ—¶è¾“å‡ºæœ€ç»ˆç»“æœå’Œæ¨ç†è¿‡ç¨‹ 3. è¾“å‡ºä¸¥æ ¼éµå¾ªJSONæ ¼å¼ä¸”ä¸å¸¦ä»»ä½•é¢å¤–æ–‡æœ¬"
                            "ç¤ºä¾‹æ ¼å¼ï¼š{\"emoji_rep\": \"1ï¸âƒ£â¤ï¸1ï¸âƒ£ğŸ‘”\", \"inference_chain\": \"æˆè¯­'ä¸€å¿ƒä¸€æ„'ç”Ÿæˆé€»è¾‘ï¼š1.1ï¸âƒ£(ä¸€)...\"}"
                            "ã€è¾“å‡ºè§„èŒƒã€‘è¯·ç¡®ä¿ï¼šâœ“ è¡¨æƒ…æ•°é‡ä¸¥æ ¼å¯¹åº”å››å­—æˆè¯­ âœ“ ä¼˜å…ˆé€‰æ‹©ç›´è§‚æ˜“æ‡‚çš„è¡¨æƒ…ç¬¦å· âœ“ å‘éŸ³åŒ¹é…éœ€æ ‡æ³¨å¯¹åº”æ‹¼éŸ³ âœ“ æ¨ç†è¿‡ç¨‹éœ€åˆ†æ­¥éª¤è¯´æ˜æ¯ä¸ªå­—ç¬¦çš„é€‰æ‹©ä¾æ®"
                        )
                    },
                    {
                        "role": "user", 
                        "content": f"è¯·ç”Ÿæˆæˆè¯­ ['{idiom}'] çš„emojiè¡¨æƒ…ç¬¦å·ç»„"
                    },
                ],
                stream=False
            )           
            # Process the response
            response_content = response.choices[0].message.content
            success = process_response(idiom, response_content)
            print(f"Success: {success}")
            
            # Flush writes to ensure data is saved
            if success:
                with open(compliant_output_file, 'a', encoding='utf-8') as f:
                    f.flush()
            else:
                with open(error_output_file, 'a', encoding='utf-8') as f:
                    f.flush()
            
        except Exception as e:
            print(f"Error: {str(e)}")
            # Save the error case
            fail_data = {
                "idiom": idiom,
                "response": f"Error: {str(e)}"
            }
            with open(error_output_file, 'a', encoding='utf-8') as f:
                json.dump(fail_data, f, ensure_ascii=False)
                f.write('\n')
                f.flush()

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nProcess interrupted by user. All completed items have been saved.")
    except Exception as e:
        print(f"\nUnexpected error: {str(e)}")
        print("All completed items have been saved.")