#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
convert_idiom_txt_to_json.py

Convert all txt files in idiom_emoji_questions to unified JSON format.
Reads txt files and creates a single JSON file with idiom-emoji pairs.
"""

import json
import os
import re
import random
from pathlib import Path
from typing import List, Dict, Tuple

def parse_txt_file(file_path: Path) -> List[Dict[str, str]]:
    """
    è§£æå•ä¸ª txt æ–‡ä»¶ï¼Œæå–æˆè¯­å’Œè¡¨æƒ…ç¬¦å·å¯¹
    
    Args:
        file_path: txt æ–‡ä»¶è·¯å¾„ï¼Œæ–‡ä»¶åæ˜¯æˆè¯­ï¼Œå†…å®¹æ˜¯è¡¨æƒ…ç¬¦å·
        
    Returns:
        æˆè¯­-è¡¨æƒ…ç¬¦å·å¯¹çš„åˆ—è¡¨
    """
    idiom_emoji_pairs = []
    
    try:
        # ä»æ–‡ä»¶åæå–æˆè¯­ï¼ˆå»æ‰.txtæ‰©å±•åï¼‰
        idiom = file_path.stem
        
        # éªŒè¯æˆè¯­æ ¼å¼ï¼ˆ4ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
        if not is_valid_idiom(idiom):
            print(f"  âš ï¸  æ–‡ä»¶åä¸æ˜¯æœ‰æ•ˆçš„å››å­—æˆè¯­: {idiom}")
            return []
        
        # è¯»å–æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        
        # æŒ‰è¡Œåˆ†å‰²å†…å®¹
        lines = content.strip().split('\n')
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            # è·³è¿‡æ ‡é¢˜è¡Œï¼ˆåŒ…å«"==="çš„è¡Œï¼‰
            if '===' in line:
                continue
            
            # æå–è¡¨æƒ…ç¬¦å·ï¼šç§»é™¤æ‰€æœ‰ä¸­æ–‡å­—ç¬¦ã€æ•°å­—ã€ç­‰å·ç­‰
            emoji_rep = extract_emojis_from_line(line)
            
            if emoji_rep:  # åªæ·»åŠ æœ‰æ•ˆçš„è¡¨æƒ…ç¬¦å·
                idiom_emoji_pairs.append({
                    "idiom": idiom,
                    "emoji_rep": emoji_rep
                })
    
    except Exception as e:
        print(f"âŒ è§£ææ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")
    
    return idiom_emoji_pairs

def extract_emojis_from_line(line: str) -> str:
    """
    ä»å•è¡Œå†…å®¹ä¸­æå–è¡¨æƒ…ç¬¦å·ï¼Œå¿½ç•¥ä¸­æ–‡å­—ç¬¦ã€æ•°å­—ã€ç­‰å·ç­‰
    
    Args:
        line: å•è¡Œå†…å®¹
        
    Returns:
        æå–å‡ºçš„è¡¨æƒ…ç¬¦å·å­—ç¬¦ä¸²
    """
    # ç§»é™¤æ‰€æœ‰ä¸­æ–‡å­—ç¬¦
    content_no_chinese = re.sub(r'[\u4e00-\u9fff]', '', line)
    
    # ç§»é™¤æ•°å­—ã€ç­‰å·ã€æ ‡ç‚¹ç¬¦å·å’Œç©ºç™½å­—ç¬¦
    content_cleaned = re.sub(r'[0-9=ï¼Œã€‚ï¼ï¼Ÿï¼šï¼›ã€\s\n\r\t]', '', content_no_chinese)
    
    # è¿›ä¸€æ­¥æ¸…ç†ï¼šåªä¿ç•™è¡¨æƒ…ç¬¦å·èŒƒå›´çš„Unicodeå­—ç¬¦
    emoji_chars = []
    for char in content_cleaned:
        code = ord(char)
        # ä¿ç•™è¡¨æƒ…ç¬¦å·å’Œç›¸å…³UnicodeèŒƒå›´ï¼Œä½†æ’é™¤æ•°å­—å’Œç­‰å·
        if (0x1F600 <= code <= 0x1F64F or  # è¡¨æƒ…
            0x1F300 <= code <= 0x1F5FF or  # æ‚é¡¹ç¬¦å·
            0x1F680 <= code <= 0x1F6FF or  # äº¤é€šç¬¦å·
            0x1F1E6 <= code <= 0x1F1FF or  # åŒºåŸŸæŒ‡ç¤ºç¬¦
            0x2600 <= code <= 0x26FF or   # æ‚é¡¹ç¬¦å·
            0x2700 <= code <= 0x27BF or   # è£…é¥°ç¬¦å·
            0x1F900 <= code <= 0x1F9FF or  # è¡¥å……ç¬¦å·
            0x1FA70 <= code <= 0x1FAFF or  # æ‰©å±•A
            0xFE00 <= code <= 0xFE0F or   # å˜ä½“é€‰æ‹©å™¨
            0x20E3 == code):               # ç»„åˆé”®ç›˜ç¬¦å·ï¼ˆä½†ä¸åŒ…å«æ•°å­—æœ¬èº«ï¼‰
            emoji_chars.append(char)
    
    return ''.join(emoji_chars)

def count_emojis(emoji_text: str) -> int:
    """
    è®¡ç®—å­—ç¬¦ä¸²ä¸­è¡¨æƒ…ç¬¦å·çš„æ•°é‡
    
    Args:
        emoji_text: åŒ…å«è¡¨æƒ…ç¬¦å·çš„å­—ç¬¦ä¸²
        
    Returns:
        è¡¨æƒ…ç¬¦å·çš„æ•°é‡
    """
    emoji_count = 0
    i = 0
    while i < len(emoji_text):
        char = emoji_text[i]
        code = ord(char)
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¡¨æƒ…ç¬¦å·
        if (0x1F600 <= code <= 0x1F64F or  # è¡¨æƒ…
            0x1F300 <= code <= 0x1F5FF or  # æ‚é¡¹ç¬¦å·
            0x1F680 <= code <= 0x1F6FF or  # äº¤é€šç¬¦å·
            0x1F1E6 <= code <= 0x1F1FF or  # åŒºåŸŸæŒ‡ç¤ºç¬¦
            0x2600 <= code <= 0x26FF or   # æ‚é¡¹ç¬¦å·
            0x2700 <= code <= 0x27BF or   # è£…é¥°ç¬¦å·
            0x1F900 <= code <= 0x1F9FF or  # è¡¥å……ç¬¦å·
            0x1FA70 <= code <= 0x1FAFF):  # æ‰©å±•A
            emoji_count += 1
            
            # è·³è¿‡å¯èƒ½çš„ä¿®é¥°ç¬¦
            if i + 1 < len(emoji_text):
                next_code = ord(emoji_text[i + 1])
                if 0xFE00 <= next_code <= 0xFE0F:  # å˜ä½“é€‰æ‹©å™¨
                    i += 1
        i += 1
    
    return emoji_count

def is_valid_idiom(text: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å››å­—æˆè¯­"""
    if not text:
        return False
    
    # ç§»é™¤æ‰€æœ‰éä¸­æ–‡å­—ç¬¦
    chinese_chars = re.findall(r'[\u4e00-\u9fff]', text)
    chinese_text = ''.join(chinese_chars)
    
    # æ£€æŸ¥æ˜¯å¦æ°å¥½æ˜¯å››ä¸ªä¸­æ–‡å­—ç¬¦
    return len(chinese_text) == 4 and chinese_text == text

def convert_all_txt_to_json():
    """
    è½¬æ¢æ‰€æœ‰ txt æ–‡ä»¶ä¸ºç»Ÿä¸€çš„ JSON æ ¼å¼
    """
    print("ğŸš€ å¼€å§‹è½¬æ¢ idiom_emoji_questions ä¸­çš„æ‰€æœ‰ txt æ–‡ä»¶...")
    
    # è®¾ç½®è·¯å¾„
    project_root = Path(__file__).parent.parent  # ä» utools å›åˆ°é¡¹ç›®æ ¹ç›®å½•
    input_dir = project_root / "data_generation_alt" / "idiom_emoji_questions"
    output_dir = project_root / "data_generation"
    output_file = output_dir / "chinese_idiom_complete.json"
    
    # æ£€æŸ¥è¾“å…¥ç›®å½•
    if not input_dir.exists():
        print(f"âŒ è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰ txt æ–‡ä»¶
    txt_files = list(input_dir.glob("*.txt"))
    
    if not txt_files:
        print(f"âŒ åœ¨ {input_dir} ä¸­æ²¡æœ‰æ‰¾åˆ° txt æ–‡ä»¶")
        return
    
    print(f"ğŸ“ æ‰¾åˆ° {len(txt_files)} ä¸ª txt æ–‡ä»¶")
    
    # è§£ææ‰€æœ‰æ–‡ä»¶å¹¶æŒ‰æˆè¯­åˆ†ç»„
    idiom_dict = {}
    idiom_index = 1
    
    for txt_file in txt_files:
        print(f"ğŸ“– å¤„ç†æ–‡ä»¶: {txt_file.name}")
        
        pairs = parse_txt_file(txt_file)
        
        if pairs:
            # å…ˆè¿‡æ»¤å‡ºæ°å¥½åŒ…å«4ä¸ªè¡¨æƒ…ç¬¦å·çš„ç»„åˆ
            valid_pairs = [pair for pair in pairs if count_emojis(pair['emoji_rep']) == 4]
            
            if not valid_pairs:
                print(f"  âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å«4ä¸ªè¡¨æƒ…ç¬¦å·çš„ç»„åˆ: {txt_file.name}")
                continue
            
            # éšæœºæŒ‘é€‰æœ€å¤š5ä¸ªè¡¨æƒ…ç»„åˆ
            if len(valid_pairs) > 5:
                selected_pairs = random.sample(valid_pairs, 5)
                print(f"  ğŸ² ä» {len(valid_pairs)} ä¸ªæœ‰æ•ˆè¡¨æƒ…ç»„åˆä¸­éšæœºé€‰æ‹©äº† 5 ä¸ª")
            else:
                selected_pairs = valid_pairs
                print(f"  âœ… ä½¿ç”¨å…¨éƒ¨ {len(valid_pairs)} ä¸ªæœ‰æ•ˆè¡¨æƒ…ç»„åˆ")
            
            # è·å–æˆè¯­åç§°
            idiom_name = selected_pairs[0]['idiom']
            
            # åˆ›å»ºæˆè¯­å¯¹è±¡ç»“æ„
            idiom_entry = {
                "idiom_index": idiom_index,
                "idiom": idiom_name,
                "emoji_rep": []
            }
            
            # æ·»åŠ è¡¨æƒ…ç»„åˆ
            for idx, pair in enumerate(selected_pairs, 1):
                # éªŒè¯è¡¨æƒ…ç¬¦å·æ•°é‡
                emoji_count = count_emojis(pair['emoji_rep'])
                
                emoji_entry = {
                    "index": idx,
                    "emoji_set": pair['emoji_rep'],
                    "homophonic_num": emoji_count
                }
                idiom_entry["emoji_rep"].append(emoji_entry)
                print(f"  âœ… {idiom_name}: {pair['emoji_rep']} (åŒ…å«{emoji_count}ä¸ªè¡¨æƒ…)")
            
            idiom_dict[idiom_name] = idiom_entry
            idiom_index += 1
        else:
            print(f"  âŒ è§£æå¤±è´¥: {txt_file.name}")
    
    # æŒ‰æˆè¯­æ’åºå¹¶è½¬æ¢ä¸ºåˆ—è¡¨
    sorted_idioms = sorted(idiom_dict.keys())
    final_result = []
    
    # é‡æ–°åˆ†é…ç´¢å¼•
    for idx, idiom_name in enumerate(sorted_idioms, 1):
        idiom_entry = idiom_dict[idiom_name]
        idiom_entry["idiom_index"] = idx
        final_result.append(idiom_entry)
    
    # ä¿å­˜ä¸º JSON
    try:
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, ensure_ascii=False, indent=4)
        
        print(f"\nâœ… è½¬æ¢å®Œæˆ!")
        print(f"ğŸ“Š æ€»å…±è½¬æ¢äº† {len(final_result)} ä¸ªæˆè¯­")
        print(f"ğŸ’¾ è¾“å‡ºæ–‡ä»¶: {output_file}")
        
        # æ˜¾ç¤ºå‰å‡ ä¸ªç¤ºä¾‹
        if final_result:
            print(f"\nğŸ“‹ å‰3ä¸ªç¤ºä¾‹:")
            for i, entry in enumerate(final_result[:3]):
                print(f"  {i+1}. {entry['idiom']} (å…±{len(entry['emoji_rep'])}ä¸ªè¡¨æƒ…ç»„åˆ)")
                for emoji_rep in entry['emoji_rep']:
                    print(f"     - {emoji_rep['emoji_set']}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ JSON æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def preview_txt_files():
    """
    é¢„è§ˆ txt æ–‡ä»¶å†…å®¹ï¼Œå¸®åŠ©ç†è§£æ–‡ä»¶æ ¼å¼
    """
    project_root = Path(__file__).parent.parent
    input_dir = project_root / "data_generation_alt" / "idiom_emoji_questions"
    
    if not input_dir.exists():
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {input_dir}")
        return
    
    txt_files = list(input_dir.glob("*.txt"))[:3]  # é¢„è§ˆå‰3ä¸ªæ–‡ä»¶
    
    for txt_file in txt_files:
        print(f"\nğŸ“– é¢„è§ˆæ–‡ä»¶: {txt_file.name}")
        print(f"   æˆè¯­: {txt_file.stem}")
        print("=" * 50)
        
        try:
            pairs = parse_txt_file(txt_file)
            print(f"æ‰¾åˆ° {len(pairs)} ä¸ªè¡¨æƒ…ç¬¦å·ç»„åˆ:")
            
            for i, pair in enumerate(pairs[:5]):  # åªæ˜¾ç¤ºå‰5ä¸ª
                print(f"  {i+1}. {pair['emoji_rep']}")
            
            if len(pairs) > 5:
                print(f"  ... è¿˜æœ‰ {len(pairs) - 5} ä¸ª")
                    
        except Exception as e:
            print(f"  âŒ è¯»å–å¤±è´¥: {e}")

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Convert idiom txt files to JSON format")
    parser.add_argument("--preview", action="store_true", help="Preview txt files content")
    parser.add_argument("--convert", action="store_true", help="Convert all txt files to JSON")
    
    args = parser.parse_args()
    
    if args.preview:
        preview_txt_files()
    elif args.convert:
        convert_all_txt_to_json()
    else:
        print("ğŸ” ä½¿ç”¨ --preview é¢„è§ˆæ–‡ä»¶å†…å®¹")
        print("ğŸ”„ ä½¿ç”¨ --convert å¼€å§‹è½¬æ¢")
        print("\nç¤ºä¾‹:")
        print("  python convert_idiom_txt_to_json.py --preview")
        print("  python convert_idiom_txt_to_json.py --convert")